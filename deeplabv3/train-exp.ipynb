{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3bb487-ec76-41a3-b97a-05ecf2bf7ab1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "/home/irfan/Desktop/Data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012\n",
      "/home/irfan/Desktop/Data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012\n",
      "Dataset: voc, Train set: 1464, Val set: 1449\n",
      "[!] Retrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/irfan/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torchvision/transforms/functional.py:417: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Itrs 10/30000, Loss=2.205916\n",
      "Epoch 1, Itrs 20/30000, Loss=1.905808\n",
      "Epoch 1, Itrs 30/30000, Loss=1.529457\n",
      "Epoch 1, Itrs 40/30000, Loss=1.304898\n",
      "Epoch 1, Itrs 50/30000, Loss=1.133827\n",
      "Epoch 1, Itrs 60/30000, Loss=1.172934\n",
      "Epoch 1, Itrs 70/30000, Loss=1.203926\n",
      "Epoch 1, Itrs 80/30000, Loss=0.970447\n",
      "Epoch 1, Itrs 90/30000, Loss=1.056333\n",
      "Epoch 1, Itrs 100/30000, Loss=1.018532\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints/exp-1/latest_deeplabv3plus_mobilenet_voc_os16.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 354\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m#torch.cuda.memory._record_memory_history()\u001b[39;00m\n\u001b[1;32m    353\u001b[0m params \u001b[38;5;241m=\u001b[39m Params()\n\u001b[0;32m--> 354\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 322\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(opts)\u001b[0m\n\u001b[1;32m    319\u001b[0m     interval_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (cur_itrs) \u001b[38;5;241m%\u001b[39m opts\u001b[38;5;241m.\u001b[39mval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[43msave_ckpt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoints/exp-1/latest_\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_os\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\n\u001b[1;32m    323\u001b[0m \u001b[43m              \u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_stride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    325\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[1], line 248\u001b[0m, in \u001b[0;36mmain.<locals>.save_ckpt\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_ckpt\u001b[39m(path):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" save current model\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcur_itrs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_itrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscheduler_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m path)\n",
      "File \u001b[0;32m~/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torch/serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[0;32m~/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/exp-1/latest_deeplabv3plus_mobilenet_voc_os16.pth'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import network\n",
    "import utils\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils import data\n",
    "from datasets import VOCSegmentation, Cityscapes\n",
    "from utils import ext_transforms as et\n",
    "from metrics import StreamSegMetrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.visualizer import Visualizer\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF']='max_split_size_mb:4096'\n",
    "\n",
    "class Params:\n",
    "    def __init__(self):\n",
    "        # Datset Options\n",
    "        self.data_root ='/home/irfan/Desktop/Data/VOCtrainval_11-May-2012'#VOCdevkit/VOC2012/'                  \n",
    "        self.dataset ='voc'\n",
    "        self.num_classes =None\n",
    "\n",
    "        # Deeplab Options\n",
    "        available_models = sorted(name for name in network.modeling.__dict__ if name.islower() and \\\n",
    "                                  not (name.startswith(\"__\") or name.startswith('_')) and callable(\n",
    "                                  network.modeling.__dict__[name])\n",
    "                                  )\n",
    "        self.model = 'deeplabv3plus_mobilenet'\n",
    "        self.separable_conv = False\n",
    "        self.output_stride  = 16\n",
    "        # Train Options\n",
    "        self.test_only      = False\n",
    "        self.save_val_results = False\n",
    "        self.total_itrs     = 30e3\n",
    "        self.lr             = 0.01\n",
    "        self.lr_policy      = 'poly'\n",
    "\n",
    "        self.step_size      = 10000\n",
    "        self.crop_val       = False\n",
    "        self.batch_size     = 8\n",
    "        self.val_batch_size = 2\n",
    "        self.crop_size      = 256\n",
    "        self.ckpt           = None\n",
    "        self.continue_training = False\n",
    "\n",
    "        self.loss_type      = 'cross_entropy'\n",
    "        self.gpu_id         = '0'\n",
    "        self.weight_decay   = 1e-4\n",
    "        self.random_seed    = 1\n",
    "        self.print_interval = 10\n",
    "        self.val_interval   = 100\n",
    "        self.download       = False\n",
    "        # PASCAL VOC Options\n",
    "        self.year           ='2012'\n",
    "\n",
    "        # Visdom options\n",
    "        self.enable_vis     = False              \n",
    "        self.vis_port       ='13570'\n",
    "        self.vis_env        ='main'\n",
    "        self.vis_num_samples =8\n",
    "        \n",
    "def get_dataset(opts):\n",
    "    \"\"\" Dataset And Augmentation\n",
    "    \"\"\"\n",
    "    if opts.dataset == 'voc':\n",
    "        train_transform = et.ExtCompose([\n",
    "            # et.ExtResize(size=opts.crop_size),\n",
    "            et.ExtRandomScale((0.5, 2.0)),\n",
    "            et.ExtRandomCrop(size=(opts.crop_size, opts.crop_size), pad_if_needed=True),\n",
    "            et.ExtRandomHorizontalFlip(),\n",
    "            et.ExtToTensor(),\n",
    "            et.ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        if opts.crop_val:\n",
    "            val_transform = et.ExtCompose([\n",
    "                et.ExtResize(opts.crop_size),\n",
    "                et.ExtCenterCrop(opts.crop_size),\n",
    "                et.ExtToTensor(),\n",
    "                et.ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        else:\n",
    "            val_transform = et.ExtCompose([\n",
    "                et.ExtToTensor(),\n",
    "                et.ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        train_dst = VOCSegmentation(root=opts.data_root, year=opts.year,\n",
    "                                    image_set='train', download=opts.download, transform=train_transform)\n",
    "        val_dst = VOCSegmentation(root=opts.data_root, year=opts.year,\n",
    "                                  image_set='val', download=False, transform=val_transform)\n",
    "\n",
    "    if opts.dataset == 'cityscapes':\n",
    "        train_transform = et.ExtCompose([\n",
    "            # et.ExtResize( 512 ),\n",
    "            et.ExtRandomCrop(size=(opts.crop_size, opts.crop_size)),\n",
    "            et.ExtColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "            et.ExtRandomHorizontalFlip(),\n",
    "            et.ExtToTensor(),\n",
    "            et.ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        val_transform = et.ExtCompose([\n",
    "            # et.ExtResize( 512 ),\n",
    "            et.ExtToTensor(),\n",
    "            et.ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        train_dst = Cityscapes(root=opts.data_root,\n",
    "                               split='train', transform=train_transform)\n",
    "        val_dst = Cityscapes(root=opts.data_root,\n",
    "                             split='val', transform=val_transform)\n",
    "    return train_dst, val_dst\n",
    "\n",
    "\n",
    "def validate(opts, model, loader, device, metrics, ret_samples_ids=None):\n",
    "    \"\"\"Do validation and return specified samples\"\"\"\n",
    "    metrics.reset()\n",
    "    ret_samples = []\n",
    "    if opts.save_val_results:\n",
    "        if not os.path.exists('results'):\n",
    "            os.mkdir('results')\n",
    "        denorm = utils.Denormalize(mean=[0.485, 0.456, 0.406],\n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "        img_id = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in tqdm(enumerate(loader)):\n",
    "\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = outputs.detach().max(dim=1)[1].cpu().numpy()\n",
    "            targets = labels.cpu().numpy()\n",
    "\n",
    "            metrics.update(targets, preds)\n",
    "            if ret_samples_ids is not None and i in ret_samples_ids:  # get vis samples\n",
    "                ret_samples.append(\n",
    "                    (images[0].detach().cpu().numpy(), targets[0], preds[0]))\n",
    "\n",
    "            if opts.save_val_results:\n",
    "                for i in range(len(images)):\n",
    "                    image = images[i].detach().cpu().numpy()\n",
    "                    target = targets[i]\n",
    "                    pred = preds[i]\n",
    "\n",
    "                    image = (denorm(image) * 255).transpose(1, 2, 0).astype(np.uint8)\n",
    "                    target = loader.dataset.decode_target(target).astype(np.uint8)\n",
    "                    pred = loader.dataset.decode_target(pred).astype(np.uint8)\n",
    "\n",
    "                    Image.fromarray(image).save('results/%d_image.png' % img_id)\n",
    "                    Image.fromarray(target).save('results/%d_target.png' % img_id)\n",
    "                    Image.fromarray(pred).save('results/%d_pred.png' % img_id)\n",
    "\n",
    "                    fig = plt.figure()\n",
    "                    plt.imshow(image)\n",
    "                    plt.axis('off')\n",
    "                    plt.imshow(pred, alpha=0.7)\n",
    "                    ax = plt.gca()\n",
    "                    ax.xaxis.set_major_locator(matplotlib.ticker.NullLocator())\n",
    "                    ax.yaxis.set_major_locator(matplotlib.ticker.NullLocator())\n",
    "                    plt.savefig('results/%d_overlay.png' % img_id, bbox_inches='tight', pad_inches=0)\n",
    "                    plt.close()\n",
    "                    img_id += 1\n",
    "\n",
    "        score = metrics.get_results()\n",
    "    return score, ret_samples\n",
    "\n",
    "\n",
    "def main(opts):\n",
    "    #opts = get_argparser().parse_args()\n",
    "    if opts.dataset.lower() == 'voc':\n",
    "        opts.num_classes = 21\n",
    "    elif opts.dataset.lower() == 'cityscapes':\n",
    "        opts.num_classes = 19\n",
    "\n",
    "    # Setup visualization\n",
    "    vis = Visualizer(port=opts.vis_port,\n",
    "                     env=opts.vis_env) if opts.enable_vis else None\n",
    "    if vis is not None:  # display options\n",
    "        vis.vis_table(\"Options\", vars(opts))\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = opts.gpu_id\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device: %s\" % device)\n",
    "\n",
    "    # Setup random seed\n",
    "    torch.manual_seed(opts.random_seed)\n",
    "    np.random.seed(opts.random_seed)\n",
    "    random.seed(opts.random_seed)\n",
    "\n",
    "    # Setup dataloader\n",
    "    if opts.dataset == 'voc' and not opts.crop_val:\n",
    "        opts.val_batch_size = 1\n",
    "\n",
    "    train_dst, val_dst = get_dataset(opts)\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dst, batch_size=opts.batch_size, shuffle=True, num_workers=2,\n",
    "        drop_last=True)  # drop_last=True to ignore single-image batches.\n",
    "    val_loader = data.DataLoader(\n",
    "        val_dst, batch_size=opts.val_batch_size, shuffle=True, num_workers=2)\n",
    "    print(\"Dataset: %s, Train set: %d, Val set: %d\" %\n",
    "          (opts.dataset, len(train_dst), len(val_dst)))\n",
    "\n",
    "    # Set up model (all models are 'constructed at network.modeling)\n",
    "    model = network.modeling.__dict__[opts.model](num_classes=opts.num_classes, output_stride=opts.output_stride)\n",
    "    if opts.separable_conv and 'plus' in opts.model:\n",
    "        network.convert_to_separable_conv(model.classifier)\n",
    "    utils.set_bn_momentum(model.backbone, momentum=0.01)\n",
    "\n",
    "    # Set up metrics\n",
    "    metrics = StreamSegMetrics(opts.num_classes)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.SGD(params=[\n",
    "        {'params': model.backbone.parameters(), 'lr': 0.1 * opts.lr},\n",
    "        {'params': model.classifier.parameters(), 'lr': opts.lr},\n",
    "    ], lr=opts.lr, momentum=0.9, weight_decay=opts.weight_decay)\n",
    "    # optimizer = torch.optim.SGD(params=model.parameters(), lr=opts.lr, momentum=0.9, weight_decay=opts.weight_decay)\n",
    "    # torch.optim.lr_scheduler.StepLR(optimizer, step_size=opts.lr_decay_step, gamma=opts.lr_decay_factor)\n",
    "    if opts.lr_policy == 'poly':\n",
    "        scheduler = utils.PolyLR(optimizer, opts.total_itrs, power=0.9)\n",
    "    elif opts.lr_policy == 'step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=opts.step_size, gamma=0.1)\n",
    "\n",
    "    # Set up criterion\n",
    "    # criterion = utils.get_loss(opts.loss_type)\n",
    "    if opts.loss_type == 'focal_loss':\n",
    "        criterion = utils.FocalLoss(ignore_index=255, size_average=True)\n",
    "    elif opts.loss_type == 'cross_entropy':\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=255, reduction='mean')\n",
    "\n",
    "    def save_ckpt(path):\n",
    "        \"\"\" save current model\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            \"cur_itrs\": cur_itrs,\n",
    "            \"model_state\": model.module.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict(),\n",
    "            \"best_score\": best_score,\n",
    "        }, path)\n",
    "        print(\"Model saved as %s\" % path)\n",
    "\n",
    "    utils.mkdir('checkpoints')\n",
    "    # Restore\n",
    "    best_score = 0.0\n",
    "    cur_itrs = 0\n",
    "    cur_epochs = 0\n",
    "    if opts.ckpt is not None and os.path.isfile(opts.ckpt):\n",
    "        # https://github.com/VainF/DeepLabV3Plus-Pytorch/issues/8#issuecomment-605601402, @PytaichukBohdan\n",
    "        checkpoint = torch.load(opts.ckpt, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "        if opts.continue_training:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "            cur_itrs = checkpoint[\"cur_itrs\"]\n",
    "            best_score = checkpoint['best_score']\n",
    "            print(\"Training state restored from %s\" % opts.ckpt)\n",
    "        print(\"Model restored from %s\" % opts.ckpt)\n",
    "        del checkpoint  # free memory\n",
    "    else:\n",
    "        print(\"[!] Retrain\")\n",
    "        model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "\n",
    "    # ==========   Train Loop   ==========#\n",
    "    vis_sample_id = np.random.randint(0, len(val_loader), opts.vis_num_samples,\n",
    "                                      np.int32) if opts.enable_vis else None  # sample idxs for visualization\n",
    "    denorm = utils.Denormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # denormalization for ori images\n",
    "\n",
    "    if opts.test_only:\n",
    "        model.eval()\n",
    "        val_score, ret_samples = validate(\n",
    "            opts=opts, model=model, loader=val_loader, device=device, metrics=metrics, ret_samples_ids=vis_sample_id)\n",
    "        print(metrics.to_str(val_score))\n",
    "        return\n",
    "\n",
    "    interval_loss = 0\n",
    "    while True:  # cur_itrs < opts.total_itrs:\n",
    "        # =====  Train  =====\n",
    "        model.train()\n",
    "        cur_epochs += 1\n",
    "        for (images, labels) in train_loader:\n",
    "            cur_itrs += 1\n",
    "\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            np_loss = loss.detach().cpu().numpy()\n",
    "            interval_loss += np_loss\n",
    "            if vis is not None:\n",
    "                vis.vis_scalar('Loss', cur_itrs, np_loss)\n",
    "\n",
    "            if (cur_itrs) % 10 == 0:\n",
    "                interval_loss = interval_loss / 10\n",
    "                print(\"Epoch %d, Itrs %d/%d, Loss=%f\" %\n",
    "                      (cur_epochs, cur_itrs, opts.total_itrs, interval_loss))\n",
    "                interval_loss = 0.0\n",
    "\n",
    "            if (cur_itrs) % opts.val_interval == 0:\n",
    "                save_ckpt('checkpoints/exp-1/latest_%s_%s_os%d.pth' %\n",
    "                          (opts.model, opts.dataset, opts.output_stride))\n",
    "                print(\"validation...\")\n",
    "                model.eval()\n",
    "                val_score, ret_samples = validate(\n",
    "                    opts=opts, model=model, loader=val_loader, device=device, metrics=metrics,\n",
    "                    ret_samples_ids=vis_sample_id)\n",
    "                print(metrics.to_str(val_score))\n",
    "                if val_score['Mean IoU'] > best_score:  # save best model\n",
    "                    best_score = val_score['Mean IoU']\n",
    "                    save_ckpt('checkpoints/exp-1/best_%s_%s_os%d.pth' %\n",
    "                              (opts.model, opts.dataset, opts.output_stride))\n",
    "\n",
    "                if vis is not None:  # visualize validation score and samples\n",
    "                    vis.vis_scalar(\"[Val] Overall Acc\", cur_itrs, val_score['Overall Acc'])\n",
    "                    vis.vis_scalar(\"[Val] Mean IoU\", cur_itrs, val_score['Mean IoU'])\n",
    "                    vis.vis_table(\"[Val] Class IoU\", val_score['Class IoU'])\n",
    "\n",
    "                    for k, (img, target, lbl) in enumerate(ret_samples):\n",
    "                        img = (denorm(img) * 255).astype(np.uint8)\n",
    "                        target = train_dst.decode_target(target).transpose(2, 0, 1).astype(np.uint8)\n",
    "                        lbl = train_dst.decode_target(lbl).transpose(2, 0, 1).astype(np.uint8)\n",
    "                        concat_img = np.concatenate((img, target, lbl), axis=2)  # concat along width\n",
    "                        vis.vis_image('Sample %d' % k, concat_img)\n",
    "                model.train()\n",
    "            scheduler.step()\n",
    "\n",
    "            if cur_itrs >= opts.total_itrs:\n",
    "                return\n",
    "\n",
    "#torch.cuda.memory._record_memory_history()\n",
    "params = Params()\n",
    "main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320e58e-a496-4874-8d19-3eb7c124b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_38",
   "language": "python",
   "name": "python_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
