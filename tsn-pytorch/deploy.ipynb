{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767c567d-8200-4fb4-a590-f650307bed62",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing TSN with base model: resnet50.\n",
      "TSN Configurations:\n",
      "    input_modality:     RGB\n",
      "    num_segments:       3\n",
      "    new_length:         1\n",
      "    consensus_module:   avg\n",
      "    dropout_ratio:      0.8\n",
      "        \n",
      "group: first_conv_weight has 1 params, lr_mult: 1, decay_mult: 1\n",
      "group: first_conv_bias has 0 params, lr_mult: 2, decay_mult: 0\n",
      "group: normal_weight has 53 params, lr_mult: 1, decay_mult: 1\n",
      "group: normal_bias has 1 params, lr_mult: 2, decay_mult: 0\n",
      "group: BN scale/shift has 2 params, lr_mult: 1, decay_mult: 0\n",
      "Freezing BatchNorm2D except the first one.\n",
      "Test: [0/946]\tTime 0.936 (0.936)\tLoss 1.3796 (1.3796)\tPrec@1 50.000 (50.000)\tPrec@5 100.000 (100.000)\n",
      "Test: [300/946]\tTime 0.030 (0.072)\tLoss 1.3883 (1.1783)\tPrec@1 25.000 (69.601)\tPrec@5 100.000 (90.698)\n",
      "Test: [600/946]\tTime 0.028 (0.072)\tLoss 0.0000 (1.4223)\tPrec@1 100.000 (64.767)\tPrec@5 100.000 (86.980)\n",
      "Test: [900/946]\tTime 0.041 (0.072)\tLoss 0.6683 (1.4092)\tPrec@1 50.000 (64.151)\tPrec@5 100.000 (86.765)\n",
      "Testing Results: Prec@1 64.658 Prec@5 87.179 Loss 1.38800\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torchvision.models import resnet101\n",
    "\n",
    "from dataset import TSNDataSet\n",
    "from models import TSN\n",
    "from transforms import *\n",
    "from opts import parser\n",
    "\n",
    "best_prec1 = 0\n",
    "\n",
    "class Params():\n",
    "    def __init__(self):\n",
    "        self.dataset = 'ucf101'\n",
    "        self.arch    = 'resnet50'\n",
    "        self.num_segments = 3\n",
    "        self.gd       = 20\n",
    "        self.lr       = 0.001\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 0.001\n",
    "        self.lr_steps = [30, 60]\n",
    "        self.epochs   = 80\n",
    "        self.b        = 8\n",
    "        self.j        = 8\n",
    "        self.dropout  = 0.8\n",
    "        self.snapshot_pref = 'ucf101_bninception_'\n",
    "        self.workers   = 2\n",
    "        self.root_path  = '/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1'\n",
    "        self.train_list = f'{self.root_path}/train.csv'\n",
    "        self.val_list   = f'{self.root_path}/test.csv'\n",
    "        self.modality   = 'RGB'\n",
    "        self.consensus_type = 'avg'\n",
    "        self.no_partialbn   = False\n",
    "        self.gpus           = [torch.device('cuda')]\n",
    "        self.device         = torch.device('cuda')\n",
    "        self.start_epoch    = 0\n",
    "        self.resume         = False\n",
    "        self.evaluate       = True\n",
    "        self.loss_type      = 'nll'\n",
    "        self.clip_gradient  = None\n",
    "        self.print_freq     = 300\n",
    "        self.eval_freq      = 1\n",
    "        \n",
    "def main():\n",
    "    global args, best_prec1\n",
    "    args = Params()#parser.parse_args()\n",
    "\n",
    "    if args.dataset == 'ucf101':\n",
    "        num_class = 101\n",
    "    elif args.dataset == 'hmdb51':\n",
    "        num_class = 51\n",
    "    elif args.dataset == 'kinetics':\n",
    "        num_class = 400\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset '+args.dataset)\n",
    "\n",
    "    model = TSN(num_class, args.num_segments, args.modality,\n",
    "                base_model=args.arch,\n",
    "                consensus_type=args.consensus_type, dropout=args.dropout, partial_bn=not args.no_partialbn)\n",
    "    \n",
    "    #https://download.pytorch.org/models/resnet101-63fe2227.pth\n",
    "    #model = resnet101(weights=torchvision.models.resnet.ResNet101_Weights)#.classifier\n",
    "    #model.fc = torch.nn.Linear(in_features=2048, out_features=101, bias=True)\n",
    "\n",
    "    crop_size = model.crop_size\n",
    "    scale_size = model.scale_size\n",
    "    input_mean = model.input_mean\n",
    "    input_std = model.input_std\n",
    "    policies = model.get_optim_policies()\n",
    "    train_augmentation = model.get_augmentation()\n",
    "\n",
    "    #model = torch.nn.DataParallel(model, device_ids=args.gpus).cuda()\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print((\"=> loading checkpoint '{}'\".format(args.resume)))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print((\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.evaluate, checkpoint['epoch'])))\n",
    "        else:\n",
    "            print((\"=> no checkpoint found at '{}'\".format(args.resume)))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    if args.modality != 'RGBDiff':\n",
    "        normalize = GroupNormalize(input_mean, input_std)\n",
    "    else:\n",
    "        normalize = IdentityTransform()\n",
    "\n",
    "    if args.modality == 'RGB':\n",
    "        data_length = 1\n",
    "    elif args.modality in ['Flow', 'RGBDiff']:\n",
    "        data_length = 5\n",
    "        \n",
    "    train_transform = torchvision.transforms.Compose([\n",
    "                       train_augmentation,\n",
    "                       Stack(roll=args.arch == 'BNInception'),\n",
    "                       ToTorchFormatTensor(div=args.arch != 'BNInception'),\n",
    "                       normalize,\n",
    "                   ])\n",
    "    val_transform = torchvision.transforms.Compose([\n",
    "                       GroupScale(int(scale_size)),\n",
    "                       GroupCenterCrop(crop_size),\n",
    "                       Stack(roll=args.arch == 'BNInception'),\n",
    "                       ToTorchFormatTensor(div=args.arch != 'BNInception'),\n",
    "                       normalize,\n",
    "                   ])\n",
    "    \n",
    "    train_dataset = TSNDataSet(args.root_path, args.train_list, num_segments=args.num_segments,\n",
    "                   new_length=data_length,\n",
    "                   modality=args.modality,\n",
    "                   image_tmpl=\"img_{:05d}.jpg\" if args.modality in [\"RGB\", \"RGBDiff\"] else args.flow_prefix+\"{}_{:05d}.jpg\",\n",
    "                   transform=train_transform)\n",
    "    val_dataset  =  TSNDataSet(args.root_path, args.val_list, num_segments=args.num_segments,\n",
    "                   new_length=data_length,\n",
    "                   modality=args.modality,\n",
    "                   image_tmpl=\"img_{:05d}.jpg\" if args.modality in [\"RGB\", \"RGBDiff\"] else args.flow_prefix+\"{}_{:05d}.jpg\",\n",
    "                   random_shift=False,\n",
    "                   transform=val_transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.b, shuffle=True,\n",
    "        num_workers=args.j, pin_memory=True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.b//2, shuffle=False,\n",
    "        num_workers=args.j, pin_memory=True)\n",
    "    \n",
    "    # define loss function (criterion) and optimizer\n",
    "    if args.loss_type == 'nll':\n",
    "        criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown loss type\")\n",
    "\n",
    "    for group in policies:\n",
    "        print(('group: {} has {} params, lr_mult: {}, decay_mult: {}'.format(\n",
    "            group['name'], len(group['params']), group['lr_mult'], group['decay_mult'])))\n",
    "\n",
    "    optimizer = torch.optim.SGD(policies,\n",
    "                                args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "    state_dict = torch.load('_ucf101_bninception__rgb_model_best.pth.tar',map_location=args.device)['state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    validate(val_loader, model, criterion, 0)\n",
    "    return model,val_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, iter, logger=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses     = AverageMeter()\n",
    "    top1       = AverageMeter()\n",
    "    top5       = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    model = model.to(args.device)\n",
    "    end = time.time()\n",
    "    for i, (input_var, target_var) in enumerate(val_loader):\n",
    "        input_var  = input_var.to(args.device)\n",
    "        target_var = target_var.to(args.device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target_var, topk=(1,5))\n",
    "\n",
    "        losses.update(loss.data, input_var.size(0))\n",
    "        top1.update(prec1, input_var.size(0))\n",
    "        top5.update(prec5, input_var.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print(('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5)))\n",
    "\n",
    "    print(('Testing Results: Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Loss {loss.avg:.5f}'\n",
    "          .format(top1=top1, top5=top5, loss=losses)))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    filename = '_'.join((args.snapshot_pref, args.modality.lower(), filename))\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        best_name = '_'.join((args.snapshot_pref, args.modality.lower(), 'model_best.pth.tar'))\n",
    "        shutil.copyfile(filename, best_name)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    #import pdb;pdb.set_trace()\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model,dataser = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a449b9-1aa1-4914-91f3-2e2b16e80c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img,lbl in dataser:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52a42591-2f61-4bb7-8fa4-bb79d5328634",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(img.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cba433d-d57d-4517-84ef-3f9b73699fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 9, 224, 224]), torch.Size([4, 101]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape,out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5368b9db-d28d-45e7-9602-9d6658e7a581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing BatchNorm2D except the first one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:785: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing BatchNorm2D except the first one.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "x = torch.randn(batch_size, 9, 224, 224, requires_grad=True)\n",
    "model = model.to('cpu')\n",
    "torch_out = model(x.to('cpu'))\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"_ucf101_bninception__rgb_model_best.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "678976c5-dda7-418e-9620-9d55c1ae54ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3433\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[31], line 1\u001b[0m\n    sm = torch.jit.script(model)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torch/jit/_script.py:1286\u001b[0m in \u001b[1;35mscript\u001b[0m\n    return torch.jit._recursive.create_script_module(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torch/jit/_recursive.py:457\u001b[0m in \u001b[1;35mcreate_script_module\u001b[0m\n    AttributeTypeIsSupportedChecker().check(nn_module)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torch/jit/_check.py:74\u001b[0m in \u001b[1;35mcheck\u001b[0m\n    init_ast = ast.parse(textwrap.dedent(source_lines))\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/usr/lib/python3.8/ast.py:47\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self, num_class, num_segments, modality,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "sm = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2034d16-fae5-49cd-aa6f-5cbf0218af18",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 98 / 101 (97%)\nMax absolute difference: 0.07035995\nMax relative difference: 2.403498\n x: array([[-0.882186,  2.370037,  4.181107,  3.86592 ,  0.129871,  1.492176,\n         0.007689, -1.299026,  0.798733, -2.507371, -2.653229, -0.514497,\n        -1.101331,  0.681547, -1.615307,  0.053302, -0.320427, -3.424017,...\n y: array([[-0.916885,  2.353652,  4.179123,  3.909896,  0.148668,  1.458838,\n         0.020534, -1.303972,  0.817456, -2.476382, -2.688036, -0.501032,\n        -1.118268,  0.700103, -1.597446,  0.058127, -0.32084 , -3.455338,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m ort_outs \u001b[38;5;241m=\u001b[39m ort_session\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mNone\u001b[39;00m, ort_inputs)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# compare ONNX Runtime and PyTorch results\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mort_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExported model has been tested with ONNXRuntime, and the result looks good!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/numpy/testing/_private/utils.py:862\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    858\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remarks)\n\u001b[1;32m    859\u001b[0m         msg \u001b[38;5;241m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    860\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39mverbose, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    861\u001b[0m                             names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m), precision\u001b[38;5;241m=\u001b[39mprecision)\n\u001b[0;32m--> 862\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 98 / 101 (97%)\nMax absolute difference: 0.07035995\nMax relative difference: 2.403498\n x: array([[-0.882186,  2.370037,  4.181107,  3.86592 ,  0.129871,  1.492176,\n         0.007689, -1.299026,  0.798733, -2.507371, -2.653229, -0.514497,\n        -1.101331,  0.681547, -1.615307,  0.053302, -0.320427, -3.424017,...\n y: array([[-0.916885,  2.353652,  4.179123,  3.909896,  0.148668,  1.458838,\n         0.020534, -1.303972,  0.817456, -2.476382, -2.688036, -0.501032,\n        -1.118268,  0.700103, -1.597446,  0.058127, -0.32084 , -3.455338,..."
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"_ucf101_bninception__rgb_model_best.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "y = torch.randn(batch_size, 9, 224, 224, requires_grad=True)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(y)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19c7ce40-8c6c-4e64-850b-9718adc8417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# An instance of your model.\n",
    "#model = torchvision.models.resnet18()\n",
    "\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(1, 9, 224, 224)\n",
    "\n",
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "traced_script_module = torch.jit.trace(model, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "619637c9-6827-4d76-82bc-1bb5794ed44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module.save(\"_ucf101_bninception__rgb_model_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3f189b4-01d7-417c-a80f-95b7eb8bc992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 224, 224)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_inputs['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39214b20-d957-4add-a0af-e49425753ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8822,  2.3700,  4.1811,  3.8659,  0.1299,  1.4922,  0.0077, -1.2990,\n",
       "           0.7987, -2.5074, -2.6532, -0.5145, -1.1013,  0.6815, -1.6153,  0.0533,\n",
       "          -0.3204, -3.4240,  1.6528, -0.3679, -1.8884,  5.0871, -1.2560, -2.6041,\n",
       "          -1.2286,  0.0591,  2.6265, -3.4647, -2.2038,  1.2696, -1.2700,  2.8332,\n",
       "          -0.0432, -1.0135,  0.2152,  3.7315, -1.7265,  1.5266,  0.5255, -0.2066,\n",
       "          -0.8698, -2.0443,  2.6716, -2.1056, -0.2042,  0.8584,  3.4560,  1.9974,\n",
       "          -0.3578, -0.6230, -0.7251,  2.2614,  7.3260,  1.3420,  0.2488, -1.1402,\n",
       "          -0.7861, -3.2351, -0.9871,  0.0257, -0.3929,  2.3378, -1.1859, -1.5633,\n",
       "           0.1496, -0.8328, -0.9717,  1.9937, -1.2638, -0.4265, -1.9989, -0.8412,\n",
       "          -0.2346, -1.4394,  1.2998, -1.1403, -0.2121,  0.7893, -1.0935, -3.1302,\n",
       "          -0.9206,  0.8007, -1.5965,  0.9180,  0.6712,  0.9593,  0.3821, -2.7236,\n",
       "           1.5280, -1.4498, -0.1849,  0.1984,  1.8024, -0.9313, -0.0371,  1.8654,\n",
       "          -1.8460, -0.4708,  0.6315, -0.9324,  2.9485]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " array([[-0.9168852 ,  2.353652  ,  4.179123  ,  3.9098957 ,  0.1486683 ,\n",
       "          1.458838  ,  0.02053375, -1.3039724 ,  0.8174558 , -2.4763825 ,\n",
       "         -2.6880357 , -0.501032  , -1.1182683 ,  0.70010334, -1.5974456 ,\n",
       "          0.05812718, -0.32083964, -3.4553385 ,  1.6343464 , -0.31730053,\n",
       "         -1.8916159 ,  5.072865  , -1.2602098 , -2.6142929 , -1.217833  ,\n",
       "          0.02699727,  2.6183455 , -3.4373722 , -2.2431796 ,  1.2878321 ,\n",
       "         -1.2911319 ,  2.8264887 , -0.06916296, -0.97627133,  0.24961649,\n",
       "          3.7832298 , -1.7300783 ,  1.5734993 ,  0.5082345 , -0.26241687,\n",
       "         -0.9052697 , -2.071127  ,  2.645374  , -2.0867717 , -0.22036286,\n",
       "          0.8150158 ,  3.438218  ,  1.9270087 , -0.34477913, -0.62585354,\n",
       "         -0.7514718 ,  2.2254426 ,  7.364775  ,  1.3852048 ,  0.26046672,\n",
       "         -1.183638  , -0.76025915, -3.234401  , -1.0446132 ,  0.02272367,\n",
       "         -0.40406856,  2.3340776 , -1.1609088 , -1.5394516 ,  0.1448521 ,\n",
       "         -0.8070399 , -0.9373644 ,  2.0111926 , -1.2218868 , -0.4042969 ,\n",
       "         -1.9914604 , -0.8447766 , -0.22755809, -1.4269719 ,  1.3191091 ,\n",
       "         -1.1271771 , -0.22411208,  0.8168203 , -1.1231241 , -3.1596317 ,\n",
       "         -0.91018707,  0.80387324, -1.5939463 ,  0.92201835,  0.6840346 ,\n",
       "          1.0095872 ,  0.42237476, -2.737261  ,  1.4916925 , -1.454677  ,\n",
       "         -0.25044814,  0.14488713,  1.8518167 , -0.9381809 ,  0.02645853,\n",
       "          1.9077986 , -1.8452712 , -0.5080649 ,  0.6112219 , -0.92774415,\n",
       "          2.9523945 ]], dtype=float32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_out,ort_outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7da4a4b-c12e-4d1f-801e-775dc69d1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx, onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9af41f-46a9-42cd-b4c4-482300ca82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4fbdf-20c8-4836-a1d0-06370316aa30",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (input, target) in enumerate(tl):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e79c2-8616-43d1-a7a9-66b071a07fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture('/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/media/v_Kayaking_g17_c01.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f89a9a-0fc6-4e11-811c-fd0cf4d4925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "success,frame = cap.read()\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783da567-96f1-4ecb-9e6f-a82d482a40d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input, target) in enumerate(val_loader):\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993316d-4b5e-4da8-bfa1-78e6dbcc78dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python main.py ucf101 RGB <ucf101_rgb_train_list> <ucf101_rgb_val_list> \\\n",
    "#   --arch BNInception --num_segments 3 \\\n",
    "#   --gd 20 --lr 0.001 --lr_steps 30 60 --epochs 80 \\\n",
    "#   -b 128 -j 8 --dropout 0.8 \\\n",
    "#   --snapshot_pref ucf101_bninception_ \n",
    "import torch\n",
    "import torchvision\n",
    "from dataset import TSNDataSet\n",
    "from models import TSN\n",
    "from transforms import Stack, ToTorchFormatTensor, IdentityTransform\n",
    "\n",
    "class Params():\n",
    "    def __init__(self):\n",
    "        self.dataset = 'ucf101'\n",
    "        self.arch    = 'BNInception'\n",
    "        self.num_segments = 3\n",
    "        self.gd       = 20\n",
    "        self.lr       = 0.001\n",
    "        self.lr_steps = [30, 60]\n",
    "        self.epochs   = 80\n",
    "        self.b        = 8\n",
    "        self.j        = 8\n",
    "        self.dropout  = 0.8\n",
    "        self.snapshot_pref = 'ucf101_bninception_'\n",
    "        self.workers   = 2\n",
    "        self.train_list = '/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/test.csv'\n",
    "        self.modality   = 'RGB'\n",
    "        self.consensus_type = 'avg'\n",
    "        self.no_partialbn   = False\n",
    "        self.gpus           = [torch.device('cuda')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4569ece-c733-474b-80e9-ad0e965aa0b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = Params()\n",
    "\n",
    "num_class = 101\n",
    "model = TSN(num_class, args.num_segments, args.modality,\n",
    "                base_model     = args.arch,\n",
    "                consensus_type = args.consensus_type, dropout = args.dropout, partial_bn = not args.no_partialbn)\n",
    "\n",
    "crop_size  = model.crop_size\n",
    "scale_size = model.scale_size\n",
    "input_mean = model.input_mean\n",
    "input_std  = model.input_std\n",
    "policies   = model.get_optim_policies()\n",
    "train_augmentation = model.get_augmentation()\n",
    "model              = torch.nn.DataParallel(model, device_ids=args.gpus).cuda()\n",
    "\n",
    "\n",
    "data_length = 1\n",
    "normalize = IdentityTransform()\n",
    "transform = torchvision.transforms.Compose([\n",
    "               train_augmentation,\n",
    "               Stack(roll=args.arch == 'BNInception'),\n",
    "               ToTorchFormatTensor(div=args.arch != 'BNInception'),\n",
    "               normalize,\n",
    "           ])\n",
    "\n",
    "dataset = TSNDataSet(\"/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1\", args.train_list, num_segments=args.num_segments,\n",
    "           new_length = data_length,\n",
    "           modality   = args.modality,\n",
    "           image_tmpl = \"img_{:05d}.jpg\" if args.modality in [\"RGB\", \"RGBDiff\"] else args.flow_prefix+\"{}_{:05d}.jpg\",\n",
    "           transform  = transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=args.b, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=1,#args.j, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed06bd-7d6b-44f8-8384-f40acf0dd934",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp,tgt in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ada2b-66ba-4cfe-8068-83ac69e94b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "inp   = inp.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8782f5a-e6dd-4152-9629-d9e35e85697e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a56ee-3e88-4258-acfd-81ce055dc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp = inp.view((-1, 3) + inp.size()[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3f6ee9-ae75-4185-a912-62a600f34c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.shape,_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b79c8-cf49-488a-84ed-f19ad430875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture('/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/media/v_Swing_g01_c04.avi')\n",
    "success,frame = cap.read()\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e249c9-14d1-43d0-8b51-b0f8805f4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/media/v_Swing_g01_c04.avi')\n",
    "success,frame = cap.read()\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb896d3-80bc-4c75-905a-fc2effc56e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        #target = target.cuda(async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d4fc7-dc59-4884-9e88-6aab2896ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.cuda(async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6f915-b6cb-4aee-8181-67c6571afd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdfacb7-fb47-432b-8915-c856f4b0c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72050d07-7e3d-409e-9351-fede1744c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img):\n",
    "    img = img + img.min()\n",
    "    img = img / img.max()\n",
    "    img = torch.permute(img,(1,2,0))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e8830-388e-4222-8733-cc7120cba143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(denormalize(x[0][:3]))\n",
    "plt.show()\n",
    "plt.imshow(denormalize(x[0][3:6]))\n",
    "plt.show()\n",
    "plt.imshow(denormalize(x[0][6:9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1536acd-5e5c-4dff-838b-c25e5b85acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4a485-dcc5-496b-9b23-3ba295c14034",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d7a47-6a18-43c2-b182-9b1773cbf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import VideoRecord\n",
    "record = VideoRecord(dataset.video_list[500],dataset.root_path)\n",
    "r = dataset._get_test_indices(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ed5c9-1e62-4540-aaec-b43fedfa915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "record.num_frames//3,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb288ad-1a91-4f4b-8deb-e8acaee11dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tick = (record.num_frames - dataset.new_length + 1) / float(dataset.num_segments)\n",
    "offsets = np.array([int(tick / 2.0 + tick * x) for x in range(dataset.num_segments)])\n",
    "tick,offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ce728-11d2-48bb-bb68-e562b9427efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.num_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe9910-c599-4e56-b086-8f9353b4169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "if record.num_frames > dataset.num_segments + dataset.new_length - 1:\n",
    "    tick = (record.num_frames - dataset.new_length + 1) / float(dataset.num_segments)\n",
    "    offsets = np.array([int(tick / 2.0 + tick * x) for x in range(dataset.num_segments)])\n",
    "else:\n",
    "    offsets = np.zeros((dataset.num_segments,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f0496-e6d1-46b5-948c-2cfce042acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "record.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8fe6f-5f28-4d6c-ab5f-f69be69b4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db66b1-9cb3-4de2-9c45-06f60b8022a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "lst = []\n",
    "for i in range(5):\n",
    "    lst+= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798166c-8858-4a29-86d7-938483b3ff25",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "root = '/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/media/'\n",
    "caps = []\n",
    "pc,fc = 0,0\n",
    "for x in open('/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/test.csv'):\n",
    "    path =x.strip().split(',')[1]\n",
    "    #print(f'{root}{path}')\n",
    "    if '.avi' in path: \n",
    "        caps += [cv2.VideoCapture(f'{root}{path}')]\n",
    "        success,_=caps[-1].read()\n",
    "        if not success:\n",
    "            fc+=1\n",
    "            print('Failed !!!',fc,path)\n",
    "        else:\n",
    "            pc+=1\n",
    "            print('Passed' , pc,path)\n",
    "        #if pc >1000 or fc > 1000: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4825dd5-2517-4d01-8b74-ae58ce89cbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3ee86-dcac-4886-9f5f-f84e67eede5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = open('/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/train.csv')\n",
    "len(x.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55817c7-8506-491e-95d1-6864e7a7b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc19fc9-039d-4c00-853f-3c3184839004",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/media/v_FieldHockeyPenalty_g09_c02.avi')\n",
    "start_frame_number = 0\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame_number)\n",
    "_,fm = cap.read()\n",
    "plt.imshow(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b676803-a3f6-47ad-acdf-2822957524ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/media/v_FieldHockeyPenalty_g09_c02.avi')\n",
    "start_frame_number = 100\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame_number)\n",
    "_,fm = cap.read()\n",
    "plt.imshow(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f8574-5913-463a-a76b-433442b198aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 50)\n",
    "_,fm = cap.read()\n",
    "plt.imshow(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efa5d3-d10d-4184-a4a0-f6302857c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py ucf101 RGB <ucf101_rgb_train_list> <ucf101_rgb_val_list> \\\n",
    "   --arch BNInception --num_segments 3 \\\n",
    "   --gd 20 --lr 0.001 --lr_steps 30 60 --epochs 80 \\\n",
    "   -b 128 -j 8 --dropout 0.8 \\\n",
    "   --snapshot_pref ucf101_bninception_ \n",
    "\n",
    "weight_url = 'bn_inception-9f5701afb96c8044.pth'\n",
    "wts = torch.utils.model_zoo.load_url(weight_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee1ae5-be9f-465f-9c75-54a03d594012",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "/home/irfan/Desktop/Data/ucf101_standard_split_1/ucf101_standard_split_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6fe83-060f-4016-aceb-c75f790acd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts['inception_4b_3x3_bn.bias'].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8b4c7-7728-4a2c-8ad7-19dbe8421b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "yaml.load('tf_model_zoo/bninception/bn_inception.yaml',Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9198a6b-4400-474f-b4e6-796989c46e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846adc7-66ed-4e9b-976d-15e14248967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_zoo.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb8da9-150b-4c2b-83a6-ace2cec0e14d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_38",
   "language": "python",
   "name": "python_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
