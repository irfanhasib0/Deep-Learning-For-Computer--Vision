{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b09b13b-915d-4ced-9e38-efbb0107403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pose_data import PoseGraph\n",
    "from training import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f6060c-0c38-4bca-a878-17e5bd2ec2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import VAE\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad9876c-bc9c-4d70-ad6c-455142367555",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(2,10)#,hidden_dims=[24,48,96,192])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a93ca0-f60b-496e-9842-fb39261a6160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=110592, out_features=10, bias=True)\n",
       "  (fc_var): Linear(in_features=110592, out_features=10, bias=True)\n",
       "  (decoder_input): Linear(in_features=10, out_features=110592, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36003c28-7922-4275-b5e3-4c36514e6bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00, 11.54it/s]\n"
     ]
    }
   ],
   "source": [
    "pg = PoseGraph()\n",
    "train_loader = DataLoader(pg,32)\n",
    "val_loader   = DataLoader(pg,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8236bbe7-4630-45b4-a75d-56325232b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "693051e3-39b1-4c0c-80a4-300e4766f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt  = elem[0].permute(0,3,1,2)\n",
    "model.to('cuda:0')\n",
    "out = model(dt[:,:2,:,:].float().to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83cba388-d041-4a27-aa9a-376b55a90da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2, 24, 18]), torch.Size([32, 2, 24, 18]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape,out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e71d2446-c5c4-4281-bae5-ec56f1f425ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model,train_loader,val_loader,loss_func=model.loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bec396a-a9af-4aeb-b9a1-2a8d876b24b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/119 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Deep-Learning-For-Computer-Vision/MotionDetection/stg-nf/training.py:141\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, log_writer, clip)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 141\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()))\n\u001b[1;32m    142\u001b[0m     log_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLL Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, losses\u001b[38;5;241m.\u001b[39mitem(), epoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader) \u001b[38;5;241m+\u001b[39m itern)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_checkpoint(epoch, filename\u001b[38;5;241m=\u001b[39mcheckpoint_filename)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b1942-6f99-4e67-bb0a-f61b3834547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpec_models.gcae.gcae import Encoder\n",
    "from gpec_models.fe.fe_model import init_fenet\n",
    "model = init_fenet(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1d080-2744-4972-a034-ce8637cb820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "_dt   = torch.tensor(dt.transpose(2,0,1)[None],dtype=torch.float32).to('cuda:0')\n",
    "model.train()\n",
    "model = model.to('cuda:0')\n",
    "out   = model(_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc696bf-aa7a-4dcb-8c8f-564c9faaf143",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068266d-948a-420b-8a5e-926be05af2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_t = dt\n",
    "fig,axes = plt.subplots(nrows=1,ncols=3,figsize=(10,3))\n",
    "axes[0].imshow(_t[:,:,0])\n",
    "axes[1].imshow(_t[:,:,1])\n",
    "axes[2].imshow(_t[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432c579-ad9c-4fcb-bdf7-02b90f1d117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_t = out[0][0].detach().cpu().numpy().transpose(1,2,0)\n",
    "fig,axes = plt.subplots(nrows=1,ncols=3,figsize=(10,3))\n",
    "axes[0].imshow(_t[:,:,0])\n",
    "axes[1].imshow(_t[:,:,1])\n",
    "axes[2].imshow(_t[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705fed6f-e9f9-40cc-99b5-d348aae91280",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt,mtd = pg.__getitem__(786)\n",
    "#_dt    = dt.transpose(1,2,0)\n",
    "dt[:,:,0]-=dt[:,:,0].min()\n",
    "dt[:,:,1]-=dt[:,:,1].min()\n",
    "dt[:,:,0]/=dt[:,:,0].max()\n",
    "dt[:,:,1]/=dt[:,:,1].max()\n",
    "dt[:,:,2]/=dt[:,:,2].max()\n",
    "fig,axes = plt.subplots(nrows=1,ncols=3,figsize=(10,4))\n",
    "axes[0].imshow(dt[:,:,0])\n",
    "axes[1].imshow(dt[:,:,1])\n",
    "axes[2].imshow(dt[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ae370-38f7-478f-a900-00e093c49ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt,mtd = pg.__getitem__(400)\n",
    "#_dt    = dt.transpose(1,2,0)\n",
    "dt[:,:,0]-=dt[:,:,0].min()\n",
    "dt[:,:,1]-=dt[:,:,1].min()\n",
    "dt[:,:,0]/=dt[:,:,0].max()\n",
    "dt[:,:,1]/=dt[:,:,1].max()\n",
    "dt[:,:,2]=0\n",
    "fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(10,4))\n",
    "axes[0].imshow(dt[:,:,0])\n",
    "axes[1].imshow(dt[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad9392-e715-4bb7-a984-05174a58c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[:,:,2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7169c-4f90-4633-8895-34b0ea681e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf82800-7cc9-40cf-ad04-18d295405068",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c834a8-d3b3-4bca-8c10-4240f0f787b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1842d35-d9bf-48a1-876d-98f49bc8238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pose(dt,mtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df458f5-da1a-4ff7-a5bd-3e7bc70ccadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "size  = dt.max().astype('int32')\n",
    "frame = np.zeros((size,size,3),dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b77ae-3e80-4b3b-9e96-32672d8aa49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb0ced-6672-46fa-934e-97f7393fb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data(_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e517339-39d8-4c0b-b358-1e0dd5cb0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "__dt[:,:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57adb8f3-10e7-4fde-b75a-5d86cc23e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "import time\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('--variational', action='store_true')\n",
    "parser.add_argument('--linear', action='store_true')\n",
    "parser.add_argument('--dataset', type=str, default='Cora',\n",
    "                    choices=['Cora', 'CiteSeer', 'PubMed'])\n",
    "parser.add_argument('--epochs', type=int, default=400)\n",
    "args = parser.parse_args()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      split_labels=True, add_negative_train_samples=False),\n",
    "])\n",
    "path = osp.join(osp.dirname(osp.realpath('.')), '..', 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, args.dataset, transform=transform)\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "class VariationalGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "\n",
    "class LinearEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv(x, edge_index)\n",
    "\n",
    "\n",
    "class VariationalLinearEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_mu = GCNConv(in_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "\n",
    "in_channels, out_channels = dataset.num_features, 16\n",
    "\n",
    "if not args.variational and not args.linear:\n",
    "    model = GAE(GCNEncoder(in_channels, out_channels))\n",
    "elif not args.variational and args.linear:\n",
    "    model = GAE(LinearEncoder(in_channels, out_channels))\n",
    "elif args.variational and not args.linear:\n",
    "    model = VGAE(VariationalGCNEncoder(in_channels, out_channels))\n",
    "elif args.variational and args.linear:\n",
    "    model = VGAE(VariationalLinearEncoder(in_channels, out_channels))\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
    "    if args.variational:\n",
    "        loss = loss + (1 / train_data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    auc, ap = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, AUC: {auc:.4f}, AP: {ap:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_38",
   "language": "python",
   "name": "python_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
