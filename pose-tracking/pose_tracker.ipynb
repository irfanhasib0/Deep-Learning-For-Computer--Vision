{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827933ec-8982-4ba3-b841-fa4e83174434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspired from -\n",
    "# https://github.com/MVIG-SJTU/AlphaPose\n",
    "# https://github.com/Hzzone/pytorch-openpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7acbc753-0fb0-47a4-a39c-0a66ebd4faba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load SE Resnet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/irfan/Desktop/Code/Linux-IO/python_38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pose model from ../models/pose/alpha_pose/fast_421_res152_256x192.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/Desktop/Code/Deep-Learning-For-Computer-Vision/pose-detection/posedet_alpha/trackers/utils/utils.py:733: UserWarning: The pretrained weights \"../models/pose/alpha_pose/osnet_ain_x1_0_msmt17_256x128_amsgrad_ep50_lr0.0015_coslr_b64_fb10_softmax_labsmth_flip_jitter.pth\" cannot be loaded, please check the key names manually (** ignored and continue **)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pose tracking...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/433 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOX-X model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 433/433 [07:43<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('posedet_alpha')\n",
    "import time\n",
    "from PIL import Image\n",
    "from detector_alpha import PoseDet\n",
    "from trackers.PoseFlow.poseflow_infer import PoseFlowWrapper\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from matplotlib import colors\n",
    "from collections import defaultdict\n",
    "from trackers.tracker_api import Tracker\n",
    "\n",
    "def hex2rgb(_hex):\n",
    "    _hex = _hex.lstrip('#')\n",
    "    return [int(_hex[i:i+2],16) for i in (0,2,4)]\n",
    "\n",
    "def process_poses(out):\n",
    "    pose = []\n",
    "    pscores = []\n",
    "    _boxes= []\n",
    "    for key in range(len(out)-1) if pose!=None else []:\n",
    "        pose.append(out[key]['keypoints'])\n",
    "        pscores.append(float(out[key]['proposal_score']))\n",
    "        _boxes.append(out[key]['bbox'])\n",
    "    _boxes = np.array(_boxes,dtype=np.float32)\n",
    "    pose   = np.array(pose,dtype=np.float32)\n",
    "    return pose, pscores, _boxes\n",
    "\n",
    "colors = [hex2rgb(val) for val in ['#FF0000','#00FF00','#0000FF','#FFFF00','#00FFFF','#FF00FF','#000000','#FFFFFF']+list(colors.TABLEAU_COLORS.values())]\n",
    "files  = glob.glob('/home/irfan/Desktop/Data/Pose_JSON_Data/ShanghaiTech/gt/test/frames/01_0015/*')\n",
    "#cap  = cv2.VideoCapture(src)\n",
    "\n",
    "demo = PoseDet()\n",
    "pf   = PoseFlowWrapper(link=25,match=0.2)\n",
    "tcr  = Tracker()\n",
    "i    = 0\n",
    "out_dict = defaultdict(dict)\n",
    "results = []\n",
    "for file in tqdm(sorted(files)):\n",
    "    i+=1\n",
    "    init_time = time.time()\n",
    "    img       = cv2.imread(file)\n",
    "    #if i<200: continue\n",
    "    \n",
    "    img   = cv2.resize(img,(256,192))\n",
    "    _im= img.copy()\n",
    "    pose,inps,boxes,cropped_boxes  = demo.detect(img,show=True)\n",
    "    \n",
    "    if pose != None and len(pose):\n",
    "        out = pf.step(img,{'result':pose,'imgname':i})\n",
    "        for key in range(len(out)-1) if pose!=None else []:\n",
    "            ps = out[key]\n",
    "            for pt in ps['keypoints']:\n",
    "                pid = ps['new_pid']\n",
    "                cv2.putText(img,f\"id : {pid}\",[int(ps['bbox'][0]),int(ps['bbox'][1]-10)],cv2.FONT_HERSHEY_COMPLEX,0.5,colors[pid],1)\n",
    "                cv2.circle(img,pt.astype('int32'),3,colors[pid],1)\n",
    "    \n",
    "            out_dict[ps['new_pid']].update( \n",
    "                {str(i-1).zfill(4)  : \n",
    "                 {'keypoints' : np.concatenate([ps['keypoints'],ps['kp_score']],axis=1).reshape(-1).tolist(),\n",
    "                  'score' : float(ps['proposal_score'])}\n",
    "                })\n",
    "    #pose, pscores, _boxes = process_poses(out)\n",
    "    #tc = tcr.update(img,inps=inps,bboxs=boxes,pose=pose,cropped_boxes=cropped_boxes,pscores=np.array(pscores,dtype=np.float32))\n",
    "    #print(tc)\n",
    "    results.append(Image.fromarray(img))\n",
    "    cv2.imshow('vis', img)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f0b8ae-f54f-4cd0-806e-2bab5d6a0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].save(\"results/result_3_50.gif\", format=\"GIF\", append_images=results[::3],save_all=True, duration=50, loop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba5369c4-8b62-47c6-afd2-37f113bca0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[::3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6c4eb-ce71-4daf-8816-09f814aeadf1",
   "metadata": {},
   "source": [
    "### Experimental codes dumped below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dabb9a6-42bf-4d89-b14f-037f4e6b2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/deep-person-reid/')\n",
    "from torchreid.utils import FeatureExtractor\n",
    "\n",
    "extractor = FeatureExtractor(\n",
    "    model_name='osnet_x1_0',\n",
    "    model_path='../models/pose/reid/model.pth.tar',\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "image_list = [\n",
    "    \n",
    "    'a/b/c/image001.jpg',\n",
    "    'a/b/c/image002.jpg',\n",
    "    'a/b/c/image003.jpg',\n",
    "    'a/b/c/image004.jpg',\n",
    "    'a/b/c/image005.jpg'\n",
    "]\n",
    "\n",
    "features = extractor(image_list)\n",
    "print(features.shape) # output (5, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb805c-5c01-42ca-9982-41c5afb8703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from posedet import model\n",
    "from posedet import util\n",
    "from posedet.body import Body\n",
    "from posedet.hand import Hand\n",
    "\n",
    "\n",
    "body_estimation = Body('../models/pose/body_pose_model.pth',device='cuda')\n",
    "\n",
    "print(f\"Torch device: {torch.cuda.get_device_name()}\")\n",
    "src = '/home/irfan/Desktop/Data/shanghaitech/testing/frames/01_0028.avi'\n",
    "cap = cv2.VideoCapture(src)\n",
    "cap.set(3, 320)\n",
    "cap.set(4, 240)\n",
    "\n",
    "class PoseDet():\n",
    "    def __init__(self):\n",
    "        self.body_estimation = Body('../models/pose/body_pose_model.pth',device='cuda')\n",
    "        #self.hand_estimation = Hand('../models/pose/hand_pose_model.pth')\n",
    "        \n",
    "while True:\n",
    "    ret, oriImg = cap.read()\n",
    "    oriImg = cv2.resize(oriImg,(480,320))\n",
    "    candidate, subset = body_estimation(oriImg)\n",
    "    canvas = copy.deepcopy(oriImg)\n",
    "    canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "    '''\n",
    "    # detect hand\n",
    "    hands_list = util.handDetect(candidate, subset, oriImg)\n",
    "\n",
    "    all_hand_peaks = []\n",
    "    for x, y, w, is_left in hands_list:\n",
    "        peaks = hand_estimation(oriImg[y:y+w, x:x+w, :])\n",
    "        peaks[:, 0] = np.where(peaks[:, 0]==0, peaks[:, 0], peaks[:, 0]+x)\n",
    "        peaks[:, 1] = np.where(peaks[:, 1]==0, peaks[:, 1], peaks[:, 1]+y)\n",
    "        all_hand_peaks.append(peaks)\n",
    "\n",
    "    canvas = util.draw_handpose(canvas, all_hand_peaks)\n",
    "    '''\n",
    "    #cv2.imshow('can', candidate[:,:,-1])#一个窗口用以显示原视频\n",
    "    #cv2.imshow('sub', subset[:,:,-1])#一个窗口用以显示原视频\n",
    "    cv2.imshow('demo', canvas)#一个窗口用以显示原视频\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b194450-0be2-4425-a5e8-42f7767050b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(_img.permute(1,2,0)+0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be289ed-0dfa-41da-bc2a-df0fb57818a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(_im)#.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32735f51-7ad8-47da-8cfc-32607d9f5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(_im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030bd69-f7f1-4c50-b2f0-c951d4c6de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89272f4-418f-4f95-8571-049d90d032c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b8435-f69f-4274-9561-0ce0e7549367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069eaa9-dfb4-4f2c-bec4-8aee549a4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "_img.min(),_img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4dd9e-06a4-4e04-94ce-696977853f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b781fa5-2545-491e-bfe0-467a1292acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e2018-3b25-4e3b-964e-a3b58a69e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.json','w') as file:\n",
    "    json.dump(out_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e2156-2acc-4d00-9298-fcbe429c589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ps['bbox'].astype('uint16') for ps in pose]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c99764-dff8-4c23-876e-156749a53796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('posedet_alpha')\n",
    "from trackers.tracker_api import Tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c90de2-fce5-4a5f-ba68-3a9da45df81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReidModels.osnet_ain import osnet_ain_x1_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cb1f7-8fe6-4f7d-bd81-494b5e06a049",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = osnet_ain_x1_0(num_classes=1,pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a5021-4122-4ba1-881e-f1c309fef3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d4656-1252-4b91-806c-07e1d28ffcbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe810bbd-569e-4712-87ae-a87fe8148200",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5,-1,-1):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229a70e-48d2-44c6-a24e-6201c1bafc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pf.track.keys():\n",
    "    print(pf.track[key].num_boxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_38",
   "language": "python",
   "name": "python_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
